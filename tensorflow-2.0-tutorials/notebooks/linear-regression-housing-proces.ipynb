{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "### Using Neural Networks\n",
    "- Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "housing.data, housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "X_train_full, y_train_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data using StandardScaler()\n",
    "#### Normalizing data leads to faster convergence of the optimizer i.e gradient descent.\n",
    "- Sklearn's StandardScaler()\n",
    " \n",
    "    - The main idea is to normalize/standardize (mean = 0 and standard deviation = 1) your \n",
    "      features/variables/columns of X before applying machine learning techniques.\n",
    "\n",
    "    - One important thing that you should keep in mind is that most (if not all) scikit-learn \n",
    "      models/classes/functions, expect as input a matrix X with dimensions/shape [number_of_samples, \n",
    "      number_of_features]. This is very important. Some other libraries expect as input the inverse.\n",
    "\n",
    "    - IMPORTNANT: StandardScaler() will normalize the features (each column of X, INDIVIDUALLY !!!) so that each \n",
    "      column/feature/variable will have mean = 0 and standard deviation = 1.\n",
    "\n",
    "    - P.S: I find the most upvoted answer on this page, wrong. I am quoting \"each value in the dataset will have \n",
    "      the sample mean value subtracted\" -- This is not true either correct.\n",
    "      \n",
    "#### The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. In case of multivariate data, this is done feature-wise (in other words independently for each column of the data). Given the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Sequential Keras model\n",
    "- loss: mse\n",
    "- optimizer: Stocahstic Gradient Descent\n",
    "- output layer: 1 neuron\n",
    "- output layer no activation function since we are modeling a linear hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 1.5298 - val_loss: 1.1409\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 2.4904 - val_loss: 0.4781\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4760 - val_loss: 0.4121\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4051 - val_loss: 0.3993\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3936 - val_loss: 0.3937\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3858 - val_loss: 0.3884\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3854 - val_loss: 0.3909\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.3865 - val_loss: 0.3936\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3787 - val_loss: 0.3832\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.3943 - val_loss: 0.3836\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3734 - val_loss: 0.3810\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3707 - val_loss: 0.3788\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3739 - val_loss: 0.3807\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3678 - val_loss: 0.3747\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3674 - val_loss: 0.3719\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3618 - val_loss: 0.3805\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3695 - val_loss: 0.3709\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.3628 - val_loss: 0.3715\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.3577 - val_loss: 0.3675\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3539 - val_loss: 0.3696\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 22us/sample - loss: 0.3371\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
